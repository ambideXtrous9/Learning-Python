{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision.datasets import ImageFolder\n",
    "import pytorch_lightning as pl\n",
    "import config\n",
    "from PIL import Image\n",
    "from torch import nn,optim\n",
    "import pytorch_lightning as pl\n",
    "import timm\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchmetrics\n",
    "from model import CNNModel\n",
    "from EfficientNetB0 import EfficientNet\n",
    "from Xception import XceptionNet\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from dataloader import LogoDataModule\n",
    "from lightning.pytorch import Trainer, seed_everything\n",
    "import config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Image Augmentations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize our data augmentation functions\n",
    "resize = transforms.Resize(size=(224,224))\n",
    "hFlip = transforms.RandomHorizontalFlip(p=0.25)\n",
    "vFlip = transforms.RandomVerticalFlip(p=0.25)\n",
    "rotate = transforms.RandomRotation(degrees=15)\n",
    "coljtr = transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.1, hue=0.1)\n",
    "raf = transforms.RandomAffine(degrees=40, translate=None, scale=(1, 2), shear=15)\n",
    "rrsc = transforms.RandomResizedCrop(size=224, scale=(0.8, 1.0))\n",
    "ccp  = transforms.CenterCrop(size=224)  # Image net standards\n",
    "nrml = transforms.Normalize([0.485, 0.456, 0.406],[0.229, 0.224, 0.225])  # Imagenet standards\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainTransforms = transforms.Compose([resize,hFlip,vFlip,rotate,raf,rrsc,ccp,coljtr,transforms.ToTensor(),nrml])\n",
    "valTransforms = transforms.Compose([resize,transforms.ToTensor(),nrml])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Data Loader for Image Folder**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogoDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, data_folder, batch_size=32, num_workers=4, val_split=0.2):\n",
    "        super(LogoDataModule, self).__init__()\n",
    "        self.data_folder = data_folder\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.val_split = val_split\n",
    "\n",
    "        self.train_transform = trainTransforms\n",
    "        self.val_transform = valTransforms\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        # Create dataset without transformations\n",
    "        self.dataset = ImageFolder(root=self.data_folder)\n",
    "\n",
    "        # Split dataset into training and validation sets\n",
    "        val_size = int(len(self.dataset) * self.val_split)\n",
    "        train_size = len(self.dataset) - val_size\n",
    "        self.train_dataset, self.val_dataset = random_split(self.dataset, [train_size, val_size])\n",
    "\n",
    "        # Apply transformations to the datasets\n",
    "        self.train_dataset.dataset.transform = self.train_transform\n",
    "        self.val_dataset.dataset.transform = self.val_transform\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True, num_workers=self.num_workers)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, batch_size=self.batch_size, shuffle=False, num_workers=self.num_workers)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, batch_size=self.batch_size, shuffle=False, num_workers=self.num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Create the Data Module**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_module = LogoDataModule(data_folder=config.DATA_FOLDER,\n",
    "                            batch_size=config.BATCH_SIZE,\n",
    "                            val_split=config.VAL_SPLIT)\n",
    "\n",
    "data_module.setup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Custom Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NQADataset(Dataset):\n",
    "  def __init__(self,data ,tokenizer ,source_max_token_len = 400,target_max_token_len = 32):\n",
    "\n",
    "    self.tokenizer = tokenizer\n",
    "    self.data = data\n",
    "    self.source_max_token_len = source_max_token_len\n",
    "    self.target_max_token_len = target_max_token_len\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.data)\n",
    "  \n",
    "  def __getitem__(self,index : int):\n",
    "    data_row = self.data.iloc[index]\n",
    "\n",
    "    source_encoding = self.tokenizer(\n",
    "        data_row['question'],\n",
    "        data_row['paragraph'],\n",
    "        max_length = self.source_max_token_len,\n",
    "        padding = \"max_length\",\n",
    "        truncation = \"only_second\",\n",
    "        return_attention_mask = True,\n",
    "        add_special_tokens = True,\n",
    "        return_tensors = \"pt\")\n",
    "    \n",
    "    target_encoding = self.tokenizer(\n",
    "        data_row['answer'],\n",
    "        max_length = self.target_max_token_len,\n",
    "        padding = \"max_length\",\n",
    "        truncation = True,\n",
    "        return_attention_mask = True,\n",
    "        add_special_tokens = True,\n",
    "        return_tensors = \"pt\")\n",
    "    \n",
    "    labels = target_encoding[\"input_ids\"]\n",
    "    labels[labels == 0] = -100\n",
    "\n",
    "    return dict(\n",
    "        answer = data_row['answer'],\n",
    "        input_ids = source_encoding['input_ids'].flatten(),\n",
    "        attention_mask = source_encoding['attention_mask'].flatten(),\n",
    "        labels = labels.flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Custom Data Module**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NQADataModule(pl.LightningDataModule):\n",
    "  \n",
    "  def __init__(self,train_df,val_df,test_df,MODEL_NAME,batch_size : int = 8,source_max_token_len : int = 400,target_max_token_len : int = 32):\n",
    "    super().__init__()\n",
    "    self.batch_size = batch_size\n",
    "    self.train_df = train_df\n",
    "    self.test_df = test_df\n",
    "    self.val_df = val_df\n",
    "    self.MODEL_NAME = MODEL_NAME\n",
    "    self.tokenizer = AutoTokenizer.from_pretrained(self.MODEL_NAME)\n",
    "    self.source_max_token_len = source_max_token_len\n",
    "    self.target_max_token_len = target_max_token_len\n",
    "\n",
    "  def setup(self,stage=None):\n",
    "    self.train_dataset = NQADataset(self.train_df,self.tokenizer,self.source_max_token_len,self.target_max_token_len)\n",
    "    self.val_dataset = NQADataset(self.val_df,self.tokenizer,self.source_max_token_len,self.target_max_token_len)\n",
    "    self.test_dataset = NQADataset(self.test_df,self.tokenizer,self.source_max_token_len,self.target_max_token_len)\n",
    "    \n",
    "\n",
    "  def train_dataloader(self):\n",
    "    return DataLoader(self.train_dataset,batch_size = self.batch_size,shuffle=True,num_workers=4)\n",
    "\n",
    "  def val_dataloader(self):\n",
    "    return DataLoader(self.val_dataset,batch_size = self.batch_size,num_workers=4)\n",
    "\n",
    "  def test_dataloader(self):\n",
    "    return DataLoader(self.test_dataset,batch_size = self.batch_size,num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_module = NQADataModule(train_df,val_df,test_df,MODEL_NAME,batch_size = BATCH_SIZE)\n",
    "data_module.setup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Model Architecture for Language Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NQAModel(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME,return_dict=True)\n",
    "\n",
    "    def forward(self,input_ids,attention_mask,labels=None):\n",
    "        output = self.model(\n",
    "            input_ids = input_ids,\n",
    "            attention_mask = attention_mask,\n",
    "            labels = labels)\n",
    "        \n",
    "        return output.loss, output.logits\n",
    "\n",
    "    def training_step(self,batch,batch_idx):\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        labels = batch['labels']\n",
    "        loss, outputs = self(input_ids,attention_mask,labels)\n",
    "        self.log(\"train_loss\",loss,prog_bar=True,logger=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self,batch,batch_idx):\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        labels = batch['labels']\n",
    "        loss, outputs = self(input_ids,attention_mask,labels)\n",
    "        self.log(\"val_loss\",loss,prog_bar=True,logger=True)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self,batch,batch_idx):\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        labels = batch['labels']\n",
    "        loss, outputs = self(input_ids,attention_mask,labels)\n",
    "        self.log(\"test_loss\",loss,prog_bar=True,logger=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return AdamW(self.parameters(),lr = 0.0001)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Model Architecture for CNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XceptionNet(pl.LightningModule):\n",
    "    def __init__(self,num_classes,lr):\n",
    "        super(XceptionNet, self).__init__()\n",
    "    \n",
    "        self.Lr = lr\n",
    "        \n",
    "        self.validation_step_outputs = []\n",
    "        \n",
    "        self.lossfn = nn.NLLLoss()\n",
    "        \n",
    "        self.acc = torchmetrics.Accuracy(task=\"multiclass\",num_classes=num_classes)\n",
    "\n",
    "        \n",
    "        self.model = timm.create_model('xception', pretrained=True)\n",
    "        self.model.aux_logits=False\n",
    "\n",
    "        # Freeze training for all layers\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        self.model.fc = nn.Sequential(\n",
    "                            nn.BatchNorm1d(self.model.fc.in_features),\n",
    "                            nn.Linear(self.model.fc.in_features, 256),\n",
    "                            nn.Dropout(0.2),\n",
    "                            nn.ReLU(inplace=True),\n",
    "                            nn.BatchNorm1d(256),\n",
    "                            nn.Linear(256, num_classes),\n",
    "                            nn.LogSoftmax(dim=1))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.model(x)\n",
    "        return out\n",
    "    \n",
    "    def training_step(self,batch,batch_idx):\n",
    "        input, label = batch\n",
    "        output = self(input)\n",
    "        loss = self.lossfn(output,label)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self,batch,batch_idx):\n",
    "        input, label = batch\n",
    "        output = self(input)\n",
    "        loss = self.lossfn(output,label)\n",
    "        self.validation_step_outputs.append(loss)\n",
    "        self.log(\"val_loss\", loss)\n",
    "        y_pred = torch.argmax(output,dim=1)\n",
    "        #pred = output.data.max(1, keepdim=True)[1]\n",
    "        self.acc.update(y_pred, label)\n",
    "        \n",
    "    def on_validation_epoch_end(self):\n",
    "        mean_val = torch.mean(torch.tensor(self.validation_step_outputs))\n",
    "        self.log('mean_val', mean_val)\n",
    "        self.validation_step_outputs.clear()  # free memory\n",
    "        val_accuracy = self.acc.compute()\n",
    "        self.log(\"val_accuracy\", val_accuracy)\n",
    "        # reset all metrics\n",
    "        self.acc.reset()\n",
    "        print(f\"\\nVal Accuracy: {val_accuracy:.4} \"\\\n",
    "        f\"Val Loss: {mean_val:.4}\")\n",
    "        \n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        return optim.AdamW(self.parameters(),lr=self.Lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(42, workers=True)\n",
    "\n",
    "model = XceptionNet(num_classes=config.NUM_CLASSES,lr=config.LR)\n",
    "\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath = 'checkpoints',\n",
    "    filename = config.CHECKPOINT_NAME,\n",
    "    save_top_k = 1,\n",
    "    verbose = True,\n",
    "    monitor = 'mean_val',\n",
    "    mode = 'min'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Trainer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = pl.Trainer(devices=-1, \n",
    "                  accelerator=\"gpu\",\n",
    "                  check_val_every_n_epoch=5,\n",
    "                  callbacks=[checkpoint_callback],\n",
    "                  max_epochs=config.MAX_EPOCHS)\n",
    "\n",
    "\n",
    "trainer.fit(model=model,datamodule=data_module)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
